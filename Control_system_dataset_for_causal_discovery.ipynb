{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33e0d75f-8a9f-4bb9-8da2-2d253fd68f71",
   "metadata": {},
   "source": [
    "\n",
    "Control system dataset for causal discovery\n",
    "===========================================\n",
    "\n",
    "*K.Rathsman*, *S.W.Mogensen* and *P.Nilsson*\n",
    "\n",
    "This dataset comprises control system data from the accelerator cryogenics plant (ACCP) at the European Spallation Source (ESS). If you use the data, please cite \n",
    "\n",
    "S.W.Mogensen, K.Rathsman, P.Nilsson, Causal discovery in a complex industrial system: A time series benchmark, in Proceedings of the 3rd Conference on Causal Learning and Reasoning (CLeaR), 2024, Available: https://doi.org/10.48550/arXiv.2310.18654\n",
    " \n",
    "The paper also contains a detailed description of the data and of the underlying system. More description, code, and help to get started can be found at https://soerenwengel.github.io/essdata\n",
    "\n",
    "The following is a brief description of the dataset as well as Python code to get started. The above paper contains a more detailed description. The dataset is intended as a causal discovery benchmark: A causal graph has been constructed based on knowledge of how the ACCP works, and this graph is found in the paper. Causal discovery methods should be able to recover this graph, and different methods can be compared using this benchmark. The exact interpretation of the causal graph is described in more detail in the above paper and its references, and a brief description of the causal graph is given in the Subsystems section below.\n",
    "\n",
    "The data was recorded during three periods of steady state operation. During each period, the control room operators did not change any of the control settings, however, control settings are different between time periods. A total of 233 different process variables (PVs) are present in the dataset. They are all measurements of physical quantities in the ACCP, e.g., temperatures and pressures. Each observation contains a measured value, a PV index as well as some metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053ae857-d033-4254-9e66-6b5be084a557",
   "metadata": {},
   "source": [
    "# DataportalClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ed976e-f57b-4743-bd11-76add5686d6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dataportal import DataportalClient;\n",
    "\n",
    "\n",
    "token = # Enter your generated JWT\n",
    "\n",
    "client = DataportalClient(token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03660e8-f13e-4eec-8680-71dbf6a00853",
   "metadata": {},
   "source": [
    "# Data files\n",
    "The dataset consists of a large number of datafiles. To tabulate the files we use the DataPortal.listFiles() method and Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35ae2a1-424e-4bb1-a32b-b3b2f41a16ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "dataset = 'ControlSystem'\n",
    "file_iterator = client.listFiles()\n",
    "file_list = list(file_iterator)\n",
    "files = pd.DataFrame.from_records(file_list, index = 'FileID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e8eaed-c454-46d5-850a-0479cf0cb0fd",
   "metadata": {},
   "source": [
    "## Operation periods\n",
    "The data was recorded during three periods of steady state operation. During each period, the control room operators did not change any of the control settings, however, control settings are different between time periods. Therefore we need to separate the files into three separate lists.\n",
    "\n",
    "The operation period is indicated by the OriginName in the file list:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de46c9f0-3b41-48b1-88b9-a10d0dff2978",
   "metadata": {},
   "outputs": [],
   "source": [
    "operation_periods = files[['OriginName']].drop_duplicates()\n",
    "operation_periods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139bf014-877e-4b0a-92b7-127f25636cde",
   "metadata": {},
   "source": [
    "To create separate file lists for the three operation periods, we use the query function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d3543c-7acd-444a-a76c-21fbef35672b",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_1 = files.query('OriginName==\"operation-period-1\"')\n",
    "files_2 = files.query('OriginName==\"operation-period-2\"')\n",
    "files_3 = files.query('OriginName==\"operation-period-3\"')\n",
    "\n",
    "files_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b32b710-e232-4002-93a3-07a894b4e9ee",
   "metadata": {},
   "source": [
    "## Initial States\n",
    "The first data files in each operation period have only a few entries (see MetricEntries) and are not complete. They have been included only to provide initial states to each process variable. To separate out these files we again use the query function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e69ad40-2ff0-433a-bcff-de6835131906",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = files_1\n",
    "\n",
    "files_initial_states = files.query('MetricEntries<=233')\n",
    "files = files.query('MetricEntries>233')\n",
    "files_initial_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc26d92-516b-4fb2-8cc0-5caf9e560aa0",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6532e3e7-0b97-49ce-91dc-728e3ec59861",
   "metadata": {},
   "source": [
    "## Initial states\n",
    "We concatenate the intial states into a dataframe for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d298aaf9-8622-4fe2-801c-2ac57980a9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_states = pd.concat([client.getData(file_id) for file_id in files_initial_states.index], axis = 0)\n",
    "for c in initial_states.columns[1:]:\n",
    "    initial_states[c] = pd.Categorical(initial_states[c], ordered = True)\n",
    "initial_states.head()\n",
    "dtypes = initial_states.dtypes.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f847e20-3ee5-4309-a510-5c6a392b3759",
   "metadata": {},
   "source": [
    "## Metadata \n",
    "A total of 233 different process variables (PVs) are present in the dataset. Each process variable has a name, unit, description, subsystem and sensor type. A table of metadata can be created from the intial states as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f608d4af-31bc-4784-a3a3-ee922401ec14",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = initial_states.drop('Value',axis='columns').set_index('Name').sort_values(['Subsystem_Index','Name'])\n",
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917c4dfa-3d83-430b-940a-46427900ed42",
   "metadata": {},
   "source": [
    "### Subsystems\n",
    "The accelerator cryogenics plant system is divided into a number of subsystems. These subsystems and their causal relations are described in the paper mentioned above. To create a table of all subsystems, including the number of process variables they represent we use pandas.value_counts():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6874c956-087e-48a2-b3d5-20f5f56bab20",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsystems = metadata[['Subsystem_Index','Subsystem_Description']].value_counts().to_frame(name='Count').reset_index('Subsystem_Description').sort_index()\n",
    "subsystems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8206e8fb-c74f-437a-9b15-9cdbbd915940",
   "metadata": {},
   "source": [
    "### Sensor types\n",
    "Process variables represent data from different sensor types, as described in the aforementioned paper. They can be listed as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0714fe0-c930-49f0-a135-36b58b555fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_types = metadata[['SensorType_Index','SensorType_Description']].value_counts().to_frame(name='Count').reset_index(['SensorType_Description' ]).sort_index()\n",
    "sensor_types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea4d779-1b1e-4666-bdf2-d6423496f7fa",
   "metadata": {},
   "source": [
    "## The actual data\n",
    "Each row in a datafile represents a state change of exactly one process variable at the time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15410d61-81fb-4c39-9da1-1abf3bdfd289",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_id = files.index[0]\n",
    "data = client.getData(file_id)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a27d054-2d74-4ac5-b8eb-507ac72a40e4",
   "metadata": {},
   "source": [
    "### Include initial states\n",
    "This step will insert the initial states into the dataframe and also create new intial states for the next data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6266f15d-9089-4112-ae5a-434252d96356",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = pd.Index([data.index[0].floor('h')]*len(initial_states), name = data.index.name)\n",
    "data = pd.concat([initial_states.set_index(start),data.astype(dtypes)])\n",
    "initial_states = data.loc[data['Name'].drop_duplicates(keep='last').index,:].copy()\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e822962e-fc25-454c-b5ff-da4535b4392c",
   "metadata": {},
   "source": [
    "### Plot\n",
    "To select and plot the measured values for, e.g., the PV 'TT-34750' from '2022-12-30 17:00' to '2022-12-30 17:10' we use pandas.plot() and populate it with some metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267a8736-2421-4aff-84da-3d67fabea332",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = 'TT-34750'\n",
    "FROM = '2022-12-30 17:00'\n",
    "TO = '2022-12-30 17:10'\n",
    "\n",
    "query = f\"Name == '{NAME}'\"\n",
    "ylabel = f\"{metadata.loc[NAME,'Description']} ({metadata.loc[NAME,'Unit']})\"\n",
    "\n",
    "value = data.query(query).loc[FROM:TO,'Value']\n",
    "ax = value.plot(ylabel=ylabel, title=NAME, marker = '.', linestyle = '', alpha = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b276849b-3eb2-4af5-9c29-8a0983162941",
   "metadata": {},
   "source": [
    "### Resample\n",
    "Changes in cryogenics systems are expected to be somewhat slow and to resample the dataset to equidistant timestamps we use Pandas.resample() method. (Note however that resampling and aggregation over time may be problematic in the context of causal discovery.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d545e8d-8602-42c0-9c91-b5cd09303c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLING_PERIOD = '10 seconds'\n",
    "\n",
    "period = pd.Timedelta(SAMPLING_PERIOD)\n",
    "resampled_data = data.groupby('Name',observed=True)[['Value']].resample(period).mean().reset_index('Name').sort_index()\n",
    "\n",
    "resampled_value = resampled_data.query(query).loc[FROM:TO,'Value']\n",
    "legend = f'{SAMPLING_PERIOD=}'\n",
    "\n",
    "ax = value.plot(ylabel=ylabel, title=NAME, marker = '.', linestyle = '', alpha = 0.2, label = 'raw_data', legend=True)\n",
    "ax = resampled_value.plot(label = legend, legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499bdd09-ba74-4f4b-9889-1d901e8b07ee",
   "metadata": {},
   "source": [
    "### Pivot\n",
    "To create a table with a common time index we use Pandas.pivot():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee915e9b-163d-47e0-8987-af085b25fb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_data = resampled_data.pivot(columns = 'Name',values='Value').ffill()\n",
    "\n",
    "columns = metadata['Subsystem_Index'].sort_values()\n",
    "pivoted_data  = pivoted_data[list(columns.index)]\n",
    "\n",
    "pivoted_data.columns = pd.MultiIndex.from_arrays([columns,columns.index])\n",
    "pivoted_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2d63b0-f334-4991-8488-5dbad8cec488",
   "metadata": {},
   "source": [
    "### Correlation Matrix\n",
    "Finally, we can construct the correlation matrix. (Note that this ignores the temporal structure of the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7901a93a-e381-4a31-8cde-e719c608993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "correlation = pivoted_data.corr()\n",
    "correlation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
